{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import nltk as nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import string\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLen(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if len(t) >= minlen and t not in ['the','and','or','is','some','.']] for d in docs ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getDocs(filename=\"train.dat\"):\n",
    "    # open docs file and read its lines\n",
    "    with open(filename, \"r\") as fh:\n",
    "        train_lines = fh.readlines()\n",
    "        \n",
    "    with open(\"English\", \"r\") as fh:\n",
    "        english_stop_words = fh.readlines()\n",
    "\n",
    "    if(filename==\"train.dat\"):\n",
    "        train_labels=  [l.split()[0] for l in train_lines]\n",
    "        train_documents=[process_text(' '.join(l.split()[1:]),english_stop_words) for l in train_lines]  \n",
    "        #train_documents=[l.split()[1:] for l in train_lines]  \n",
    "        #train_documents=filterLen(train_documents,2)\n",
    "\n",
    "\n",
    "    \n",
    "    return train_labels, train_documents\n",
    "\n",
    "\n",
    "def getTestDocs(filename=\"test.dat\"):\n",
    "    # open docs file and read its lines\n",
    "    with open(filename, \"r\") as fh:\n",
    "        test_lines = fh.readlines()\n",
    "    #train_documents=[process_text(' '.join(l.split()[1:])) for l in train_lines]  \n",
    "\n",
    "    test_documents=[process_text(' '.join(l.split()[1:]),english_stop_words) for l in test_lines]  \n",
    "    \n",
    "    #test_documents=[l.split() for l in test_lines]  \n",
    "    #train_documents=filterLen(test_documents,2)\n",
    "    \n",
    "    return test_documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kmer(k,list):\n",
    "    f = []\n",
    "    for dna in list: \n",
    "        dna=dna.lower()\n",
    "        for x in range(len(dna)+1-k):\n",
    "           \n",
    "            kmer = dna[x:x+k]\n",
    "            f.append(kmer)\n",
    "    return f\n",
    "\n",
    "\n",
    "def process_text(text,english_stop_words, stem=True, length=2, c=5):\n",
    "    \"\"\" Tokenize text and stem words removing punctuation \"\"\"\n",
    "    \n",
    "    text = BeautifulSoup(text,\"lxml\").text\n",
    "    \n",
    "    tokens = text.split()\n",
    "    #s.translate(None, string.punctuation)\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "       \n",
    "    filtered_words = [word for word in tokens if word not in english_stop_words and len(word)>2]\n",
    "    filtered_words = kmer(c, filtered_words)\n",
    "\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'movie', u'slowe', u'lower', u'molas', u'olass', u'lasse', u'asses', u'janua', u'anuar', u'nuary', u'uary.', u'ary..', u'ry...', u'alask', u'laska', u'aska.', u'togea', u'ogeat', u'geath', u'eathe', u'ather', u'previ', u'revie', u'eview', u'shoul', u'hould', u'award', u'manag', u'anagi', u'nagin', u'aging', u'every', u'secon', u'econd', u'conds', u'inter', u'nteri', u'teris', u'erist', u'risti', u'istin', u'sting', u'previ', u'revie', u'eview', u'view.', u'peopl', u'eople', u'watch', u'atchi', u'tchin', u'ching', u'with,', u'sever', u'evera', u'veral', u'times', u'imes.', u'after', u'over,', u'havin', u'aving', u'woken', u'taken', u'hopin', u'oping', u'somet', u'ometh', u'methi', u'ethin', u'thing', u'actua', u'ctual', u'tuall', u'ually', u'happe', u'appen', u'ppen,', u'nothi', u'othin', u'thing', u'does.', u'loose', u'track', u'peopl', u'eople', u\"ople'\", u\"ple's\", u'motiv', u'otive', u'tives', u'ives,', u'chara', u'harac', u'aract', u'racte', u'acter', u'cters', u'unint', u'ninte', u'inter', u'nteri', u'teris', u'erist', u'risti', u'istin', u'sting', u'ting.', u'movie', u'ovie,', u'hoped', u'every', u'veryo', u'eryon', u'ryone', u'would', u'died.', u'every', u'veryo', u'eryon', u'ryone', u'aroun', u'round', u'eithe', u'ither', u'being', u'conte', u'ontem', u'ntemp', u'tempt', u'empti', u'mptib', u'ptibl', u'tible', u'ible,', u'petty', u'etty,', u'pitif', u'itifu', u'tiful', u'iful,', u'usual', u'suall', u'ually', u'three', u'hree.', u'worse', u'orse,', u'watch', u'atche', u'tched', u'minut', u'inute', u'added', u'featu', u'eatur', u'ature', u'tures', u'ures,', u'kicks', u'giggl', u'iggle', u'ggles', u'under', u'nders', u'derst', u'ersta', u'rstan', u'stand', u'tand,', u'peopl', u'eople', u'being', u'about', u'socia', u'ocial', u'ciall', u'ially', u'aware', u'spend', u'movie', u'patti', u'attin', u'tting', u'thems', u'hemse', u'emsel', u'mselv', u'selve', u'elves', u'back,', u'might', u'worth', u'watch', u'atchi', u'tchin', u'ching', u'hing.', u'broug', u'rough', u'ought', u'expec', u'xpect', u'pecti', u'ectin', u'cting', u'excit', u'xcite', u'citem', u'iteme', u'temen', u'ement', u\"'24.'\", u'lectu', u'ectur', u'cture', u'socia', u'ocial', u'aware', u'waren', u'arene', u'renes', u'eness', u'throu', u'hroug', u'rough', u'blery', u'sandm', u'andma', u'ndman', u'dman.'], [u'inter', u'ntere', u'teres', u'erest', u'resti', u'estin', u'sting', u'about', u'actua', u'ctual', u'event', u'place', u'durin', u'uring', u'civil', u'vermo', u'ermon', u'rmont', u'mont.', u'atten', u'ttent', u'tenti', u'entio', u'ntion', u\"don't\", u'regre', u'egret', u'viewi', u'iewin', u'ewing', u'haven', u\"aven'\", u\"ven't\", u'incid', u'ncide', u'ciden', u'ident', u'curio', u'uriou', u'rious', u'rebel', u'ebels', u'pulle', u'ulled', u'enjoy', u'histo', u'istor', u'stori', u'toric', u'orica', u'rical', u'films', u'era.m', u'ra.my', u'major', u'compl', u'ompla', u'mplai', u'plain', u'laint', u'confe', u'onfed', u'nfede', u'feder', u'edera', u'derat', u'erate', u'unifo', u'nifor', u'iform', u'forms', u'orms.', u'good!', u'ood!!', u'od!!!', u'actin', u'cting', u'littl', u'ittle', u'stiff', u'times', u'imes.', u'eatin', u'ating', u'mashe', u'ashed', u'potat', u'otato', u'tatoe', u'atoes', u'becau', u'ecaus', u'cause', u\"didn'\", u\"idn't\", u'teeth', u'eeth.', u'wound', u'ounde', u'unded', u'soldi', u'oldie', u'ldier', u'playi', u'layin', u'aying', u'fetch', u'hound', u'littl', u'ittle', u'stran', u'trang', u'range', u'ange.', u'overa', u'veral', u'erall', u'rall,', u'desce', u'escen', u'scent', u'film.'], [u'painf', u'ainfu', u'inful', u'nfull', u'fully', u'obvio', u'bviou', u'vious', u'peopl', u'eople', u'\"movi', u'movie', u'ovie\"', u'never', u'brill', u'rilli', u'illia', u'llian', u'liant', u'spoof', u'poofs', u'naked', u'shots', u'hots\"', u'ots\".', u'movie', u'terri', u'errib', u'rribl', u'rible', u'ible,', u'actor', u'ctors', u'tors.', u'would', u'ouldn', u\"uldn'\", u\"ldn't\", u'actin', u'cting', u'face,', u'doing', u'while', u'watch', u'atchi', u'tchin', u'ching', u'total', u'rubbi', u'ubbis', u'bbish', u'bish.', u'ish.t', u'sh.th', u'h.the', u'movie', u'stupi', u'tupid', u'humor', u'ever.', u'could', u'bette', u'etter', u'movie', u'frien', u'riend', u'iends', u'ends.', u'amazi', u'mazin', u'azing', u'movie', u'much.', u'singl', u'ingle', u'cleve', u'lever', u'funny', u'line.', u'trace', u'intel', u'ntell', u'telli', u'ellig', u'llige', u'ligen', u'igenc', u'gence', u'behin', u'ehind', u'pathe', u'athet', u'theti', u'hetic', u'movie', u'perso', u'erson', u'actua', u'ctual', u'tuall', u'ually', u'likes', u'movie', u'ovie.', u'yuck!'], [u'movie', u'reall', u'eally', u'mixed', u'hand,', u'story', u'conce', u'oncep', u'ncept', u'movie', u'reall', u'eally', u'good,', u'tense', u'twist', u'wists', u'again', u'other', u'hand,', u'slow,', u'witho', u'ithou', u'thout', u'style', u'uninv', u'ninvo', u'invol', u'nvolv', u'volve', u'olved', u'lved.', u'still', u'regar', u'egard', u'\"just', u'cause', u'ause\"', u'above', u'avera', u'verag', u'erage', u'thril', u'hrill', u'rille', u'iller', u'simpl', u'imply', u'becau', u'ecaus', u'cause', u'cast.', u'ast.m', u'st.ma', u't.may', u'.mayb', u'maybe', u'conne', u'onner', u'nnery', u'misca', u'iscas', u'scast', u'role.', u'mean,', u\"isn't\", u'reall', u'eally', u'belie', u'eliev', u'lieva', u'ievab', u'evabl', u'vable', u\"'hero\", u\"hero'\", u'fathe', u'ather', u'young', u'daugh', u'aught', u'ughte', u'ghter', u'(play', u'playe', u'layed', u'still', u'young', u'scarl', u'carle', u'arlet', u'rlett', u'johan', u'ohans', u'hanss', u'ansso', u'nsson', u'husba', u'usban', u'sband', u'capsh', u'apsha', u'pshaw', u'shaw.', u'simpl', u'imply', u'reall', u'eally', u'credi', u'redib', u'edibl', u'dible', u'howev', u'oweve', u'wever', u'conne', u'onner', u'nnere', u'nerey', u'cours', u'ourse', u'great', u'actor', u'reaso', u'eason', u'still', u'carry', u'movie', u'does.', u'cours', u'ourse', u'helpe', u'elped', u'solid', u'suppo', u'uppor', u'pport', u'porti', u'ortin', u'rting', u'consi', u'onsis', u'nsist', u'sists', u'actor', u'ctors', u'laure', u'auren', u'urenc', u'rence', u'fishb', u'ishbu', u'shbur', u'hburn', u'burne', u'urne,', u'blair', u'under', u'nderw', u'derwo', u'erwoo', u'rwood', u'wood,', u'beatt', u'eatty', u'atty,', u'lange', u'ange,', u'lynne', u'thigp', u'higpe', u'igpen', u'harri', u'arris', u'rris.', u'actor', u'ctors', u'reall', u'eally', u'highl', u'ighly', u'under', u'nderu', u'derus', u'eruse', u'rused', u'time,', u'which', u'shame', u'hame,', u'misse', u'issed', u'oppor', u'pport', u'portu', u'ortun', u'rtuni', u'tunit', u'unity', u'nity.', u'espec', u'speci', u'pecia', u'ecial', u'ciall', u'ially', u'harri', u'arris', u'total', u'otall', u'tally', u'great', u'psych', u'sycho', u'ychop', u'chopa', u'hopat', u'opath', u'pathi', u'athic', u'seria', u'erial', u'kille', u'iller', u'ller.', u'truly', u'chill', u'hilli', u'illin', u'lling', u'actin', u'cting', u'super', u'uperb', u'perbl', u'erbly', u'rbly.', u'norma', u'ormal', u'rmall', u'mally', u'doesn', u\"oesn'\", u\"esn't\", u'ruthl', u'uthle', u'thles', u'hless', u'less,', u'chill', u'hilli', u'illin', u'lling', u'roles', u'movie', u'ovies', u'vies,', u'reall', u'eally', u'surpr', u'urpri', u'rpris', u'prise', u'rises', u'perfo', u'erfor', u'rform', u'forma', u'orman', u'rmanc', u'mance', u'alone', u'alrea', u'lread', u'ready', u'enoug', u'nough', u'reaso', u'eason', u'watch', u'movie', u'ovie.', u'howev', u'oweve', u'wever', u'story', u'witho', u'ithou', u'thout', u'style', u'formu', u'ormul', u'rmula', u'mulai', u'ulaic', u'laic,', u'chara', u'harac', u'aract', u'racte', u'acter', u'cters', u'movie', u'reall', u'eally', u'becau', u'ecaus', u'cause', u'feels', u'dista', u'istan', u'stant', u'tant.', u'ant.i', u'nt.it', u'reall', u'eally', u'story', u'toryt', u'oryte', u'rytel', u'ytell', u'telli', u'ellin', u'lling', u'kills', u'movie', u'poten', u'otent', u'tenti', u'entia', u'ntial', u'tial.', u'glimc', u'limch', u'imche', u'mcher', u'direc', u'irect', u'rects', u'movie', u'littl', u'ittle', u'style', u'keeps', u'times', u'imes.', u'becau', u'ecaus', u'cause', u'this,', u'viewe', u'iewer', u'ewers', u'wers,', u'never', u'reall', u'eally', u'invol', u'nvolv', u'volve', u'olved', u'story', u'chara', u'harac', u'aract', u'racte', u'acter', u'cters', u'ters.', u'ers.i', u'rs.it', u'reall', u'eally', u'\"just', u'cause', u'ause\"', u'enoug', u'nough', u'poten', u'otent', u'tenti', u'entia', u'ntial', u'tial.', u'slick', u'story', u'unexp', u'nexpe', u'expec', u'xpect', u'pecte', u'ected', u'twist', u'wists', u'turns', u'which', u'nothi', u'othin', u'thing', u'seems', u'eems.', u'story', u'reaso', u'eason', u'movie', u'still', u'above', u'avera', u'verag', u'erage', u'thril', u'hrill', u'rille', u'iller', u'ller,', u'proba', u'robab', u'obabl', u'bably', u'still', u'pleas', u'lease', u'genre', u'enre.', u'howev', u'oweve', u'wever', u'etern', u'terna', u'ernal', u'shame', u'movie', u'lacki', u'ackin', u'cking', u'story', u'telli', u'ellin', u'lling', u'style', u'tyle,', u'movie', u'could', u'class', u'lassi', u'assic', u'genre', u'enre.', u'nre.7', u're.7/', u'e.7/1', u'.7/10'], [u'retur', u'eturn', u'movie', u'story', u'inspi', u'nspir', u'spira', u'pirat', u'irati', u'ratio', u'ation', u'famil', u'amily', u'appea', u'ppeal', u'peals', u'ages.', u'story', u'tory,', u'thoug', u'hough', u'seemi', u'eemin', u'eming', u'mingl', u'ingly', u'impos', u'mposs', u'possi', u'ossib', u'ssibl', u'sible', u'ible,', u'aspir', u'spire', u'pires', u'divin', u'ivine', u'inter', u'nterv', u'terve', u'erven', u'rvent', u'venti', u'entio', u'ntion', u'loose', u'ooses', u'tragi', u'ragic', u'accid', u'ccide', u'ciden', u'ident', u'finds', u'again', u'woman', u'recei', u'eceiv', u'ceive', u'eives', u\"wife'\", u\"ife's\", u'heart', u'eart.', u'david', u'ducho', u'uchov', u'chovn', u'hovny', u'minni', u'innie', u'drive', u'river', u'heart', u'earte', u'arted', u'perfo', u'erfor', u'rform', u'forma', u'orman', u'rmanc', u'mance', u'ances', u'desig', u'esign', u'signa', u'ignat', u'gnate', u'nated', u'to-be', u'o-be-', u'-be-l', u'be-lo', u'e-lov', u'-love', u'lover', u'overs', u'chanc', u'hance', u'ance.', u'story', u'frien', u'riend', u'iends', u'famil', u'amily', u'aroun', u'round', u'suppo', u'uppor', u'pport', u'times', u'trial', u'rial.', u'carol', u\"o'con\", u\"'conn\", u'conno', u'onnor', u'minni', u'innie', u'drive', u'river', u\"iver'\", u\"ver's\", u'grand', u'randf', u'andfa', u'ndfat', u'dfath', u'fathe', u'ather', u'ther,', u'authe', u'uthen', u'thent', u'henti', u'entic', u'every', u'scene', u'cene.', u'bonni', u'onnie', u'frien', u'riend', u'whose', u'encou', u'ncour', u'coura', u'ourag', u'urage', u'ragem', u'ageme', u'gemen', u'ement', u'under', u'nderl', u'derli', u'erlin', u'rline', u'lines', u'minni', u'innie', u\"'sist\", u'siste', u'ister', u\"ster'\", u'funny', u'scene', u'cenes', u'espec', u'speci', u'pecia', u'ecial', u'ciall', u'ially', u'james', u'belus', u'elush', u'lushi', u'husba', u'usban', u'sband', u'band.', u'class', u'lassi', u'assic', u'scene', u'cenes', u'writi', u'ritin', u'iting', u'makes', u'story', u'enjoy', u'njoya', u'joyab', u'oyabl', u'yable', u'touch', u'ouchi', u'uchin', u'ching', u'watch', u'again', u'gain.', u'thank', u'makin', u'aking', u'movie', u'demon', u'emons', u'monst', u'onstr', u'nstra', u'strat', u'trate', u'rates', u'famil', u'amili', u'milie', u'ilies', u'frien', u'riend', u'iends', u'close', u'carin', u'aring', u'peopl', u'eople', u'other', u'throu', u'hroug', u'rough', u'diffi', u'iffic', u'fficu', u'ficul', u'icult', u'times', u'imes.'], [u'spoil', u'poile', u'oiler', u'ilers', u'below', u'elow.', u'low.t', u'ow.th', u'w.the', u'prosp', u'rospe', u'ospec', u'spect', u'clear', u'learl', u'early', u'horiz', u'orizo', u'rizon', u'filme', u'ilmed', u'lmed.', u'openi', u'penin', u'ening', u'scene', u'europ', u'urope', u'ropea', u'opean', u'refug', u'efuge', u'fugee', u'ugees', u'final', u'predi', u'redic', u'edict', u'dicti', u'ictio', u'ction', u'nazii', u'aziis', u'ziism', u'death', u'milli', u'illio', u'llion', u'lions', u'germa', u'erman', u'rmans', u'mans,', u'movie', u'propa', u'ropag', u'opaga', u'pagan', u'agand', u'ganda', u'films', u'after', u'pearl', u'harbo', u'arbor', u'rbor.', u'there', u\"isn't\", u'enter', u'ntert', u'terta', u'ertai', u'rtain', u'tainm', u'ainme', u'inmen', u'nment', u'value', u'thoug', u'hough', u'foota', u'ootag', u'otage', u'inter', u'ntere', u'teres', u'erest', u'resti', u'estin', u'sting', u'those', u\"aren'\", u\"ren't\", u'enoug', u'nough', u'remem', u'ememb', u'membe', u'ember', u'prett', u'retty', u'forge', u'orget', u'rgett', u'getta', u'ettab', u'ttabl', u'table', u'dockt', u'ockto', u'cktor', u'cobur', u'oburn', u'prett', u'retty', u'accen', u'ccent', u'daugh', u'aught', u'ughte', u'ghter', u'assim', u'ssimi', u'simil', u'imila', u'milat', u'ilati', u'latin', u'ating', u'ameri', u'meric', u'erica', u'wayne', u\"ayne'\", u\"yne's\", u'help.', u'other', u'scene', u'cenes', u'enes,', u'memor', u'emora', u'morab', u'orabl', u'rable', u'aspec', u'spect', u'movie', u'viewe', u'iewed', u'hinds', u'indsi', u'ndsig', u'dsigh', u'sight', u'ight.', u'cobur', u'oburn', u\"burn'\", u\"urn's\", u'speec', u'peech', u'compa', u'ompar', u'mpari', u'parin', u'aring', u'nazii', u'aziis', u'ziism', u'malig', u'align', u'ligna', u'ignan', u'gnanc', u'nancy', u'worse', u'cance', u'ancer', u'descr', u'escri', u'scrib', u'cribi', u'ribin', u'ibing', u'(then', u'curre', u'urren', u'rrent', u'rent)', u'succe', u'ucces', u'ccess', u'cesse', u'esses', u'momen', u'oment', u'menta', u'entar', u'ntary', u'outbu', u'utbur', u'tburs', u'burst', u'energ', u'nergy', u'patie', u'atien', u'tient', u'right', u'befor', u'efore', u'death', u'eeril', u'erily', u'accur', u'ccura', u'curat', u'urate', u'varno', u\"arno'\", u\"rno's\", u'scher', u'chere', u'herer', u'playe', u'layed', u'accur', u'ccura', u'curat', u'urate', u'ratel', u'ately', u'newsr', u'ewsre', u'wsree', u'sreel', u'foota', u'ootag', u'otage', u'unrep', u'nrepe', u'repen', u'epent', u'penta', u'entan', u'ntant', u'nazis', u'justi', u'ustif', u'stify', u'tifyi', u'ifyin', u'fying', u'their', u'actio', u'ction', u'tions', u'ions.', u'ons.w', u'ns.wh', u's.whe', u'.when', u'viewe', u'iewed', u'histo', u'istor', u'stori', u'toric', u'orica', u'rical', u'persp', u'erspe', u'rspec', u'spect', u'pecti', u'ectiv', u'ctive', u'tive,', u'aspec', u'spect', u'pects', u'inter', u'ntere', u'teres', u'erest', u'resti', u'estin', u'sting', u'ting.', u'enter', u'ntert', u'terta', u'ertai', u'rtain', u'tainm', u'ainme', u'inmen', u'nment', u'outsi', u'utsid', u'tside', u'persp', u'erspe', u'rspec', u'spect', u'pecti', u'ectiv', u'ctive', u'tive,', u\"you'd\", u'wayne', u\"ayne'\", u\"yne's\", u'succe', u'ucces', u'ccess', u'cessf', u'essfu', u'ssful', u'effor', u'ffort', u'forts', u'orts.'], [u\"'what\", u\"need'\", u\"eed',\", u'every', u'episo', u'pisod', u'isode', u'since', u'lonel', u'onely', u\"nely'\", u'winne', u'inner', u'exten', u'xtent', u'tent.', u'episo', u'pisod', u'isode', u'first', u'major', u'failu', u'ailur', u'ilure', u'since', u\"'esca\", u'escap', u'scape', u'claus', u'lause', u\"ause'\", u\"use'.\", u'serli', u'erlin', u'rling', u'scrip', u'cript', u'again', u'based', u'someo', u'omeon', u'meone', u\"else'\", u\"lse's\", u'mater', u'ateri', u'terie', u'eriel', u'riel,', u'short', u'story', u'lewis', u'padge', u'adget', u'dgett', u'gett.', u'opene', u'pened', u\"ened'\", u\"ned',\", u'serli', u'erlin', u'rling', u'alter', u'ltere', u'tered', u'conte', u'onten', u'ntent', u'signi', u'ignif', u'gnifi', u'nific', u'ifica', u'fican', u'icant', u'cantl', u'antly', u'ntly,', u'remov', u'emovi', u'movin', u'oving', u'scien', u'cient', u'ienti', u'entis', u'ntist', u'machi', u'achin', u'chine', u'inser', u'nsert', u'serti', u'ertin', u'rting', u'elder', u'lderl', u'derly', u'peddl', u'eddle', u'ddler', u'dler.', u\"ler.'\", u\"er.'w\", u\"r.'wh\", u\".'wha\", u\"'what\", u\"need'\", u'works', u'being', u'sweet', u'weet.', u'openi', u'penin', u'ening', u'half,', u'which', u'peddl', u'eddle', u'ddler', u'provi', u'rovid', u'ovide', u'vides', u'custo', u'ustom', u'stome', u'tomer', u'omers', u'objec', u'bject', u'jects', u'futur', u'uture', u'ture,', u'gentl', u'entle', u'charm', u'about', u'throu', u'hroug', u'rough', u'ougho', u'ughou', u'ghout', u'entir', u'ntire', u'episo', u'pisod', u'isode', u'works', u'frame', u'allot', u'llott', u'lotte', u'otted', u'tted.', u'sadly', u'adly,', u'which', u'gapin', u'aping', u'holes', u'oles.', u'minut', u'inute', u'steve', u'cochr', u'ochra', u'chran', u\"hran'\", u\"ran's\", u'perfo', u'erfor', u'rform', u'forma', u'orman', u'rmanc', u'mance', u'two-b', u'wo-bi', u'o-bit', u'becom', u'ecome', u'comes', u'focus', u'episo', u'pisod', u'isode', u'falls', u'apart', u'part.', u'cochr', u'ochra', u'chran', u\"hran'\", u\"ran's\", u'under', u'nderw', u'derwr', u'erwri', u'rwrit', u'writt', u'ritte', u'itten', u'stere', u'tereo', u'ereot', u'reoty', u'eotyp', u'otype', u'perfo', u'erfor', u'rform', u'forma', u'orman', u'rmanc', u'mance', u'highl', u'ighli', u'ghlig', u'hligh', u'light', u'ights', u'flaw.', u'explo', u'xploi', u'ploit', u'loita', u'oitat', u'itati', u'tatio', u'ation', u'peddl', u'eddle', u'ddler', u'predi', u'redic', u'edict', u'dicta', u'ictab', u'ctabl', u'table', u'revel', u'evela', u'velat', u'elati', u'latio', u'ation', u'murde', u'urder', u'total', u'otall', u'tally', u'uncon', u'nconv', u'convi', u'onvin', u'nvinc', u'vinci', u'incin', u'ncing', u'cing,', u'makin', u'aking', u'whole', u'slipp', u'lippe', u'ipper', u'ppery', u'shoes', u'scene', u'compl', u'omple', u'mplet', u'plete', u'letel', u'etely', u'false', u'alse.', u'ernes', u'rnest', u'truex', u'peddl', u'eddle', u'ddler', u'dler,', u'bring', u'ringi', u'ingin', u'nging', u'magic', u'agica', u'gical', u'ical,', u'myste', u'yster', u'steri', u'terio', u'eriou', u'rious', u'chara', u'harac', u'aract', u'racte', u'acter', u'cter,', u'enoug', u'nough', u'floun', u'lound', u'ounde', u'under', u'nderi', u'derin', u'ering', u'scrip', u'cript', u'ript.', u'ipt.t', u'pt.to', u'matte', u'atter', u'tters', u'worse', u'orse,', u'scrip', u'cript', u'incon', u'ncons', u'consi', u'onsis', u'nsist', u'siste', u'isten', u'stenc', u'tenci', u'encie', u'ncies', u'cies.', u'insta', u'nstan', u'stanc', u'tance', u'ance,', u'learn', u'peddl', u'eddle', u'ddler', u\"dler'\", u\"ler's\", u'power', u'provi', u'rovid', u'ovide', u'peopl', u'eople', u'stems', u'abili', u'bilit', u'ility', u'futur', u'uture', u'ture.', u'exact', u'xactl', u'actly', u'allow', u'produ', u'roduc', u'oduce', u'magic', u'agica', u'gical', u'icall', u'cally', u'winni', u'innin', u'nning', u'horse', u'orses', u'rses.', u'seems', u'shoul', u'hould', u'littl', u'ittle', u'outsi', u'utsid', u'tside', u'realm', u'ealms', u'power', u'ower.', u'also,', u'futur', u'uture', u'ture,', u'peddl', u'eddle', u'ddler', u'certa', u'ertai', u'rtain', u'tainl', u'ainly', u'surpr', u'urpri', u'rpris', u'prise', u'rised', u'waiti', u'aitin', u'iting', u'flat.', u'there', u'holes', u'picke', u'icked', u\"'what\", u\"need'\", u'hardl', u'ardly', u'worth', u'episo', u'pisod', u'isode', u'throu', u'hroug', u'rough', u'anywa', u'nyway', u'yway.'], [u'looke', u'ooked', u'paper', u'aper,', u'per,n', u'er,ni', u'r,nic', u',nick', u'jerry', u'buckh', u'uckhe', u'ckhei', u'kheim', u'heime', u'eimer', u'colla', u'ollab', u'llabo', u'labor', u'abora', u'borat', u'orate', u'again', u'gain,', u'heist', u'movie', u'ovie,', u'vinci', u'code,', u'ode,a', u'de,am', u'e,ame', u',amer', u'ameri', u'meric', u'erica', u'rican', u'histo', u'istor', u'story', u'india', u'ndian', u'diana', u'jones', u'ones.', u'dear,', u'india', u'ndian', u'diana', u'jones', u'speed', u'speed', u'peed.', u'reaso', u'eason', u'asona', u'sonab', u'onabl', u'nable', u'cast(', u'ast(i', u'st(in', u't(inc', u'(incl', u'inclu', u'nclud', u'cludi', u'ludin', u'uding', u'voigh', u'oight', u'harve', u'arvey', u'keite', u'eitel', u'itel)', u'battl', u'attle', u'ttles', u'again', u'gains', u'ainst', u'pueri', u'ueril', u'erile', u'scrip', u'cript', u'loses', u'badly', u'adly.', u'littl', u'ittle', u'exten', u'xtend', u'tende', u'ended', u'adver', u'dvert', u'freem', u'reema', u'eemas', u'emaso', u'mason', u'asons', u'sons.', u'ons.h', u'ns.ho', u's.how', u'.howe', u'howev', u'oweve', u'wever', u'these', u'freem', u'reema', u'eemas', u'emaso', u'mason', u'asons', u'usual', u'shopk', u'hopke', u'opkee', u'pkeep', u'keepe', u'eeper', u'epers', u'funny', u'hands', u'andsh', u'ndsha', u'dshak', u'shake', u'hakes', u'golf,', u'these', u'freem', u'reema', u'eemas', u'emaso', u'mason', u'asons', u'natur', u'atura', u'tural', u'desce', u'escen', u'scend', u'cenda', u'endan', u'ndant', u'dants', u'knigh', u'night', u'ights', u'templ', u'empla', u'mplar', u'nobod', u'obody', u'menti', u'entio', u'ntion', u\"'from\", u\"hell'\", u'rippe', u'ipper', u'pper.', u'per.)', u'er.)i', u\"don't\", u'think', u'revea', u'eveal', u'veale', u'ealed', u'spoil', u'poile', u'oiler', u'ilers', u'becau', u'ecaus', u'cause', u'there', u'none.', u'there', u'virtu', u'irtua', u'rtual', u'tuall', u'ually', u'suspe', u'uspen', u'spens', u'pense', u'ense,', u'surpr', u'urpri', u'rpris', u'prise', u'rises', u'clima', u'limax', u'imax-', u'stops', u'tops.', u'natio', u'ation', u'tiona', u'ional', u'treas', u'reasu', u'easur', u'asure', u'brown', u'intel', u'ntell', u'telle', u'ellec', u'llect', u'lectu', u'ectua', u'ctual', u'level', u'episo', u'pisod', u'isode', u'scoob', u'cooby', u'humou', u'umour', u'mour.'], [u'fail.', u'regis', u'egist', u'giste', u'ister', u'stere', u'tered', u'garba', u'arbag', u'rbage', u'bage.', u'wrist', u'rist.', u'heres', u'paste', u'lines', u'ines.', u'fail.', u'regis', u'egist', u'giste', u'ister', u'stere', u'tered', u'garba', u'arbag', u'rbage', u'bage.', u'wrist', u'rist.', u'heres', u'paste', u'lines', u'ines.', u'fail.', u'regis', u'egist', u'giste', u'ister', u'stere', u'tered', u'garba', u'arbag', u'rbage', u'bage.', u'wrist', u'rist.', u'heres', u'paste', u'lines', u'ines.', u'fail.', u'regis', u'egist', u'giste', u'ister', u'stere', u'tered', u'garba', u'arbag', u'rbage', u'bage.', u'wrist', u'rist.', u'heres', u'paste', u'lines', u'ines.', u'fail.', u'regis', u'egist', u'giste', u'ister', u'stere', u'tered', u'garba', u'arbag', u'rbage', u'bage.', u'wrist', u'rist.', u'heres', u'paste', u'lines', u'ines.']]\n"
     ]
    }
   ],
   "source": [
    "train_labels, train_documents = getDocs()\n",
    "\n",
    "\n",
    "\n",
    "print train_documents[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_documents = getTestDocs(\"test.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print len(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        #print cnt\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        #print keys\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat,idx\n",
    "\n",
    "\n",
    "def build_test_matrix(docs,idx):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    \n",
    "    \n",
    "    nnz = 0\n",
    "    test_docs = []\n",
    "    for d in docs:\n",
    "        copy_d=[]\n",
    "     \n",
    "        for w in d:\n",
    "            if w in idx:\n",
    "                copy_d.append(w)\n",
    "        \n",
    "        nnz += len(set(copy_d))\n",
    "        test_docs.append(copy_d)\n",
    "        \n",
    "        \n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in test_docs:\n",
    "        cnt = Counter(d)\n",
    "        #print cnt\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        #print keys\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)))\n",
    "        \n",
    "        \n",
    "def splitData(mat, cls, fold=1, d=10):\n",
    "    r\"\"\" Split the matrix and class info into train and test data using d-fold hold-out\n",
    "    \"\"\"\n",
    "    n = mat.shape[0]\n",
    "    r = int(np.ceil(n*1.0/d))\n",
    "    mattr = []\n",
    "    clstr = []\n",
    "    # split mat and cls into d folds\n",
    "    for f in range(d):\n",
    "        if f+1 != fold:\n",
    "            mattr.append( mat[f*r: min((f+1)*r, n)] )\n",
    "            clstr.extend( cls[f*r: min((f+1)*r, n)] )\n",
    "    # join all fold matrices that are not the test matrix\n",
    "    train = sp.vstack(mattr, format='csr')\n",
    "    # extract the test matrix and class values associated with the test rows\n",
    "    test = mat[(fold-1)*r: min(fold*r, n), :]\n",
    "    clste = cls[(fold-1)*r: min(fold*r, n)]\n",
    "\n",
    "    return train, clstr, test, clste\n",
    "\n",
    "def classifyNames(names, cls, c=3, k=3, d=10):\n",
    "    r\"\"\" Classify names using c-mer frequency vector representations of the names and kNN classification with \n",
    "    cosine similarity and 10-fold cross validation\n",
    "    \"\"\"\n",
    "    docs = [cmer(n, c) for n in names]\n",
    "    mat = build_matrix(docs)\n",
    "    # since we're using cosine similarity, normalize the vectors\n",
    "    csr_l2normalize(mat)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat,word_dict = build_matrix(train_documents)\n",
    "csr_l2normalize(mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 276290)\n"
     ]
    }
   ],
   "source": [
    "tmat = build_test_matrix(test_documents,word_dict)\n",
    "csr_l2normalize(tmat)\n",
    "print tmat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 276290)\n"
     ]
    }
   ],
   "source": [
    "print mat.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNeighborsWhole(trainingSet, testSet, k):\n",
    "    distances = []\n",
    "    num_rows, num_cols = testSet.shape\n",
    "    labels=[]\n",
    "    distances=testSet.dot(trainingSet.T).todense()\n",
    "    #distances=testSet.dot(trainingSet.T)\n",
    "    distances =np.array(distances)\n",
    "    indixes= np.argsort(-distances,axis=1)\n",
    "    indixes=np.array(indixes)\n",
    "\n",
    " \n",
    "    for row in range(num_rows):\n",
    "        poscount=0\n",
    "        negcount=0\n",
    "        l=indixes[row]\n",
    "        d=distances[row]\n",
    "\n",
    "        for x in range(k):\n",
    "            dist=d[x]\n",
    "\n",
    "            if(dist>0):\n",
    "                if(train_labels[l[x]]=='-1'):\n",
    "                    negcount+=1  \n",
    "                else:\n",
    "                    poscount+=1\n",
    "        label='+1'\n",
    "        if(negcount>poscount):\n",
    "            label='-1'\n",
    "        elif(negcount==poscount):\n",
    "            label=train_labels[l[0]]\n",
    "            \n",
    "        labels.append(label)\n",
    "    return labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5--->0.532\n",
      "6--->0.528\n",
      "7--->0.516\n",
      "8--->0.548\n",
      "9--->0.544\n",
      "10--->0.52\n",
      "11--->0.484\n",
      "12--->0.54\n",
      "13--->0.52\n",
      "14--->0.52\n",
      "15--->0.524\n",
      "16--->0.58\n",
      "17--->0.612\n",
      "18--->0.544\n",
      "19--->0.572\n",
      "20--->0.572\n",
      "21--->0.6\n",
      "22--->0.604\n",
      "23--->0.624\n",
      "24--->0.572\n",
      "25--->0.616\n",
      "26--->0.624\n",
      "27--->0.644\n",
      "28--->0.652\n",
      "29--->0.572\n",
      "30--->0.612\n",
      "31--->0.648\n",
      "32--->0.672\n",
      "33--->0.648\n",
      "34--->0.68\n",
      "35--->0.66\n",
      "36--->0.704\n",
      "37--->0.656\n",
      "38--->0.712\n",
      "39--->0.684\n",
      "40--->0.68\n",
      "41--->0.708\n",
      "42--->0.652\n",
      "43--->0.688\n",
      "44--->0.708\n",
      "45--->0.716\n",
      "46--->0.7\n",
      "47--->0.704\n",
      "48--->0.664\n",
      "49--->0.704\n",
      "50--->0.732\n",
      "51--->0.74\n",
      "52--->0.748\n",
      "53--->0.752\n",
      "54--->0.736\n",
      "55--->0.748\n",
      "56--->0.736\n",
      "57--->0.736\n",
      "58--->0.752\n",
      "59--->0.752\n",
      "60--->0.748\n",
      "61--->0.724\n",
      "62--->0.76\n",
      "63--->0.716\n",
      "64--->0.724\n",
      "65--->0.776\n",
      "66--->0.756\n",
      "67--->0.688\n",
      "68--->0.78\n",
      "69--->0.756\n",
      "70--->0.716\n",
      "71--->0.736\n",
      "72--->0.744\n",
      "73--->0.768\n",
      "74--->0.732\n",
      "75--->0.776\n",
      "76--->0.708\n",
      "77--->0.752\n",
      "78--->0.752\n",
      "79--->0.764\n",
      "80--->0.752\n",
      "81--->0.768\n",
      "82--->0.732\n",
      "83--->0.78\n",
      "84--->0.788\n",
      "85--->0.764\n",
      "86--->0.712\n",
      "87--->0.792\n",
      "88--->0.76\n",
      "89--->0.748\n",
      "90--->0.732\n",
      "91--->0.764\n",
      "92--->0.76\n",
      "93--->0.772\n",
      "94--->0.712\n",
      "95--->0.708\n",
      "96--->0.776\n",
      "97--->0.796\n",
      "98--->0.752\n",
      "99--->0.8\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-29f1f23d1a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mclste\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclspr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m            \u001b[0macc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m    \u001b[0macc\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m    \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"--->\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m    \u001b[0mmacc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "    \n",
    "def classify(x, train, clstr,k=3):\n",
    "        r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "        \"\"\"\n",
    "        # find nearest neighbors for x\n",
    "        dots = x.dot(train.T)\n",
    "        sims = list(zip(dots.indices, dots.data))\n",
    "        if len(sims) == 0:\n",
    "            # could not find any neighbors\n",
    "            return '+' if np.random.rand() > 0.5 else '-'\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        tc = Counter(clstr[s[0]] for s in sims[:k]).most_common(2)\n",
    "        if len(tc) < 2 or tc[0][1] > tc[1][1]:\n",
    "            # majority vote\n",
    "            return tc[0][0]\n",
    "        # tie break\n",
    "        tc = defaultdict(float)\n",
    "        for s in sims[:k]:\n",
    "            tc[clstr[s[0]]] += s[1]\n",
    "        return sorted(tc.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        \n",
    "\n",
    "        \n",
    "macc = 0.0\n",
    "d=100\n",
    "\n",
    "\n",
    "\n",
    "#svd = TruncatedSVD(n_components=5000, n_iter=7, random_state=42)\n",
    "\n",
    "#scaled_mat = svd.fit_transform(mat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for f in range(5,200):\n",
    "    \n",
    "    \n",
    "        # split data into training and testing\n",
    "    train, clstr, test, clste = splitData(mat, train_labels, f+1, d)\n",
    "        # predict the class of each test sample\n",
    "    clspr = getNeighborsWhole(train,test,f)\n",
    "    #clspr = [ classify(test[i,:], train, clstr,19) for i in range(test.shape[0]) ]\n",
    "        # compute the accuracy of the prediction\n",
    "    acc = 0.0\n",
    "    for i in range(len(clste)):\n",
    "        if clste[i] == clspr[i]:\n",
    "            acc += 1\n",
    "    acc /= len(clste)\n",
    "    print str(f)+\"--->\"+str(acc)\n",
    "    macc += acc\n",
    "        \n",
    "print macc/195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "import os\n",
    "filename = 'output.dat'\n",
    "highscore = open(filename,'w')\n",
    "num_rows, num_cols = tmat.shape\n",
    "print num_rows\n",
    "\n",
    "clspr = getNeighborsWhole(mat,tmat,99)\n",
    "\n",
    "for e in clspr:\n",
    "    \n",
    "    highscore.write(e+\"\\n\")\n",
    "\n",
    "highscore.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hhh', 'hhe', 'hhl', 'hhl', 'hho', 'heh', 'hee', 'hel', 'hel', 'heo', 'hlh', 'hle', 'hll', 'hll', 'hlo', 'hlh', 'hle', 'hll', 'hll', 'hlo', 'hoh', 'hoe', 'hol', 'hol', 'hoo', 'ehh', 'ehe', 'ehl', 'ehl', 'eho', 'eeh', 'eee', 'eel', 'eel', 'eeo', 'elh', 'ele', 'ell', 'ell', 'elo', 'elh', 'ele', 'ell', 'ell', 'elo', 'eoh', 'eoe', 'eol', 'eol', 'eoo', 'lhh', 'lhe', 'lhl', 'lhl', 'lho', 'leh', 'lee', 'lel', 'lel', 'leo', 'llh', 'lle', 'lll', 'lll', 'llo', 'llh', 'lle', 'lll', 'lll', 'llo', 'loh', 'loe', 'lol', 'lol', 'loo', 'lhh', 'lhe', 'lhl', 'lhl', 'lho', 'leh', 'lee', 'lel', 'lel', 'leo', 'llh', 'lle', 'lll', 'lll', 'llo', 'llh', 'lle', 'lll', 'lll', 'llo', 'loh', 'loe', 'lol', 'lol', 'loo', 'ohh', 'ohe', 'ohl', 'ohl', 'oho', 'oeh', 'oee', 'oel', 'oel', 'oeo', 'olh', 'ole', 'oll', 'oll', 'olo', 'olh', 'ole', 'oll', 'oll', 'olo', 'ooh', 'ooe', 'ool', 'ool', 'ooo']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import product\n",
    "from string import ascii_lowercase\n",
    "keywords = map(''.join, product(\"hello\", repeat=3))\n",
    "print keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat1 =[[1,2][3,4]]\n",
    "mat2 =[[8,9],[5,7],[1,2]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "distances=mat1.dot(mat2.T)\n",
    "distances =np.array(distances)\n",
    "indixes= np.argsort(distances,axis=1)[::-1]\n",
    "\n",
    "print distances\n",
    "\n",
    "print indixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[818 635 181 636 368]\n",
      " [ 60  43  11  46  40]]\n",
      "[[0 3 1 4 2]\n",
      " [0 3 1 4 2]]\n",
      "(2, 5)\n",
      "[0 3 1 4 2]\n",
      "[818 635 181 636 368]\n",
      "818\n",
      "0\n",
      "1\n",
      "0\n",
      "635\n",
      "3\n",
      "1\n",
      "1\n",
      "181\n",
      "1\n",
      "1\n",
      "2\n",
      "[0 3 1 4 2]\n",
      "[60 43 11 46 40]\n",
      "60\n",
      "0\n",
      "1\n",
      "0\n",
      "43\n",
      "3\n",
      "1\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "2\n",
      "['+1', '+1']\n"
     ]
    }
   ],
   "source": [
    "mat1 =np.array([[1,90],[3,4]])\n",
    "mat2 =np.array([[8,9],[5,7],[1,2],[6,7],[8,4]])\n",
    "train_labels=['-1','+1','-1','+1','-1']\n",
    "\n",
    "clspr = getNeighborsWhole(mat2,mat1,3)\n",
    "\n",
    "print clspr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
