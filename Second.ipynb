{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import norm\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import metrics\n",
    "from stemming.porter2 import stem\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import nltk as nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import string\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def filterLen(docs, minlen):\n",
    "    \"\"\" filter out terms that are too short.\n",
    "    docs is a list of lists, each inner list is a document represented as a\n",
    "    list of words minlen is the minimum length of the word to keep\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(\"stop-words.txt\", \"r\") as fh:\n",
    "        stop_words = fh.readlines()\n",
    "        \n",
    "    st_list=[]\n",
    "    for s in stop_words:\n",
    "        s=s.replace(\"\\n\", \"\")\n",
    "        st_list.append(s)\n",
    "    \n",
    "    return [ [t for t in d if (len(t) >= minlen and t not in st_list) ] for d in docs ]\n",
    "\n",
    "def stemDoc(docs):\n",
    "    \"\"\" automatically removes suffixes (and in some cases prefixes) in order to\n",
    "    find the root word or stem of a given word\n",
    "    \"\"\"\n",
    "    \n",
    "    return [ [stem(t) for t in d ] for d in docs]\n",
    "\n",
    "\n",
    "def getPositiveNegativeWords():\n",
    "    with open(\"negative-words.txt\", \"r\") as fh:\n",
    "        negative_words = fh.readlines()\n",
    "    with open(\"positive-words.txt\", \"r\") as fh:\n",
    "        positive_words = fh.readlines()\n",
    "\n",
    "    \n",
    "    pos_list=[]\n",
    "    for s in positive_words:\n",
    "        s=s.replace(\"\\n\", \"\")\n",
    "        pos_list.append(s)\n",
    "    \n",
    "    pos_list1= [ stem(d) for d in pos_list] \n",
    "    pos_list.extend(pos_list1)\n",
    "    \n",
    "    \n",
    "    neg_list=[]\n",
    "    for s in negative_words:\n",
    "        s=s.replace(\"\\n\", \"\")\n",
    "        neg_list.append(s)\n",
    "    \n",
    "    neg_list1= [ stem(d) for d in neg_list] \n",
    "    neg_list.extend(neg_list1)\n",
    "    \n",
    "    return pos_list,neg_list\n",
    "        \n",
    "        \n",
    "                  \n",
    "\n",
    "def getDocs(filename=\"train.dat\"):\n",
    "    # open docs file and read its lines\n",
    "    with open(filename, \"r\") as fh:\n",
    "        train_lines = fh.readlines()\n",
    "        \n",
    "    pos_list, neg_list = getPositiveNegativeWords()\n",
    "\n",
    "    if(filename==\"train.dat\"):\n",
    "        train_labels=  [l.split()[0] for l in train_lines]\n",
    "        train_doc = [re.sub(r'[^\\w]', ' ',l[2:].lower()).split() for l in train_lines]\n",
    "        train_rev = filterLen(train_doc, 4)\n",
    "        train_documents = stemDoc(train_rev)\n",
    "        print len(train_documents[0])\n",
    "        for t in train_documents:\n",
    "            pos_count=0\n",
    "            neg_count=0\n",
    "            new_term=\"ann-neutral\"\n",
    "\n",
    "            for d in t:\n",
    "                if(d in pos_list):\n",
    "                    pos_count+=1\n",
    "                elif(d in neg_list):\n",
    "                    neg_count+=1\n",
    "            if(neg_count>pos_count):\n",
    "                new_term=\"ann-neg\"\n",
    "            elif(neg_count<pos_count):\n",
    "                new_term=\"ann-pos\"\n",
    "                \n",
    "                \n",
    "            new_list = get_k_mers(t)\n",
    "            t.append(new_term)\n",
    " \n",
    "            t.extend(new_list)\n",
    "        \n",
    "        print len(train_documents[0])\n",
    "\n",
    "    return train_labels, train_documents, pos_list,neg_list \n",
    "\n",
    "def splitTrainData(train_docs,train_labels):\n",
    "    \n",
    "    train_docs1 = train_docs[0:len(train_docs)/2]\n",
    "    train_labels1 = train_labels[0:len(train_labels)/2]\n",
    "    \n",
    "    test_docs = train_docs[len(train_docs)/2:]\n",
    "    test_labels = train_labels[len(train_labels)/2:]\n",
    "\n",
    "    return train_docs1, train_labels1, test_docs, test_labels\n",
    "\n",
    "\n",
    "\n",
    "def getTestDocs(filename=\"test.dat\"):\n",
    "    # open docs file and read its lines\n",
    "    with open(filename, \"r\") as fh:\n",
    "        test_lines = fh.readlines()\n",
    "\n",
    "\n",
    "    test_doc = [re.sub(r'[^\\w]', ' ',l[2:].lower()).split() for l in test_lines]\n",
    "    test_rev = filterLen(test_doc, 4)\n",
    "    test_documents = stemDoc(test_rev)\n",
    "    print len(test_documents[0])\n",
    "    \n",
    "    for t in test_documents:\n",
    "        pos_count=0\n",
    "        neg_count=0\n",
    "        new_term=\"ann-neutral\"\n",
    "        for d in t:\n",
    "            if(d in pos_list):\n",
    "                pos_count+=1\n",
    "            elif(d in neg_list):\n",
    "                neg_count+=1\n",
    "        if(neg_count>pos_count):\n",
    "                new_term=\"ann-neg\"\n",
    "        elif(neg_count<pos_count):\n",
    "                new_term=\"ann-pos\"\n",
    "        new_list = get_k_mers(t)\n",
    "        t.append(new_term)\n",
    "        t.extend(new_list)\n",
    "        \n",
    "    print len(test_documents[0])\n",
    "        \n",
    "    return test_documents\n",
    "\n",
    "\n",
    "# Group words for different K values for K-mer implementation\n",
    "def grouper(input_list, n = 2):\n",
    "    for i in xrange(len(input_list) - (n - 1)):\n",
    "        yield input_list[i:i+n]\n",
    "\n",
    "def get_k_mers(input_list):\n",
    "    new_list = []\n",
    "    #new_list.extend(input_list)\n",
    "    for first, second in grouper(input_list, 2):\n",
    "        st = first + \" \"+second\n",
    "        new_list.append(st)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def kmer(lists,k=3):\n",
    "    f = []\n",
    "    for word in lists:       \n",
    "        for x in range(len(word)+1-k):  \n",
    "            kmer = word[x:x+k]\n",
    "            f.append(kmer)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print train_documents[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "train_labels, train_documents,pos_list,neg_list  = getDocs()\n",
    "\n",
    "\n",
    "\n",
    "print train_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_documents, train_labels, test_documents, test_labels=splitTrainData(train_documents,train_labels)\n",
    "train_documents.extend(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_documents = getTestDocs(\"test.dat\")\n",
    "train_documents.extend(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(test_documents)\n",
    "print len(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_matrix(docs, pos_list,neg_list, weight=2):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "        \n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 2\n",
    "    nnz = 0\n",
    "    idx[\"ann-neg\"]=0\n",
    "    idx[\"ann-pos\"]=1\n",
    "\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        #print cnt\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        #print keys\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "            if(k in pos_list or k in neg_list):\n",
    "                val[j+n] = val[j+n] * weight\n",
    "            if(k =='ann-neg' or k =='ann-pos'):\n",
    "                val[j+n]=val[j+n]*15\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat,idx\n",
    "\n",
    "\n",
    "def build_test_matrix(docs,pos_list,neg_list,idx, weight=3):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "\n",
    "    \n",
    "    nnz = 0\n",
    "    test_docs = []\n",
    "    for d in docs:\n",
    "        copy_d=[]\n",
    "     \n",
    "        for w in d:\n",
    "            if w in idx:\n",
    "                copy_d.append(w)\n",
    "        \n",
    "        nnz += len(set(copy_d))\n",
    "        test_docs.append(copy_d)\n",
    "        \n",
    "        \n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in test_docs:\n",
    "        cnt = Counter(d)\n",
    "        #print cnt\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        #print keys\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "            if(k in pos_list or k in neg_list):\n",
    "                val[j+n] = val[j+n] * weight  \n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf.\n",
    "    Returns scaling factors as dict. If copy is True,\n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        if i>1:\n",
    "            df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(2, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "\n",
    "    return df if copy is False else mat\n",
    "\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done csr\n"
     ]
    }
   ],
   "source": [
    "csr_mat,word_dict = build_matrix(train_documents,pos_list,neg_list)\n",
    "\n",
    "print \"done csr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t15.0\n",
      "  (0, 2)\t2.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 7)\t2.0\n",
      "  (0, 8)\t2.0\n",
      "  (0, 9)\t2.0\n",
      "  (0, 10)\t2.0\n",
      "  (0, 11)\t1.0\n",
      "  (0, 12)\t1.0\n",
      "  (0, 13)\t1.0\n",
      "  (0, 14)\t2.0\n",
      "  (0, 15)\t2.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 17)\t4.0\n",
      "  (0, 18)\t1.0\n",
      "  (0, 19)\t1.0\n",
      "  (0, 20)\t1.0\n",
      "  (0, 21)\t4.0\n",
      "  (0, 22)\t1.0\n",
      "  (0, 23)\t4.0\n",
      "  (0, 24)\t1.0\n",
      "  (0, 25)\t2.0\n",
      "  :\t:\n",
      "  (0, 252)\t1.0\n",
      "  (0, 253)\t1.0\n",
      "  (0, 254)\t1.0\n",
      "  (0, 255)\t1.0\n",
      "  (0, 256)\t1.0\n",
      "  (0, 257)\t1.0\n",
      "  (0, 258)\t1.0\n",
      "  (0, 259)\t1.0\n",
      "  (0, 260)\t1.0\n",
      "  (0, 261)\t1.0\n",
      "  (0, 262)\t1.0\n",
      "  (0, 263)\t1.0\n",
      "  (0, 264)\t1.0\n",
      "  (0, 265)\t1.0\n",
      "  (0, 266)\t1.0\n",
      "  (0, 267)\t1.0\n",
      "  (0, 268)\t1.0\n",
      "  (0, 269)\t1.0\n",
      "  (0, 270)\t1.0\n",
      "  (0, 271)\t1.0\n",
      "  (0, 272)\t1.0\n",
      "  (0, 273)\t1.0\n",
      "  (0, 274)\t1.0\n",
      "  (0, 275)\t1.0\n",
      "  (0, 276)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print csr_mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = csr_idf(csr_mat, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat=csr_l2normalize(mat1,copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmat = build_test_matrix(test_documents,word_dict)\n",
    "csr_l2normalize(tmat)\n",
    "print tmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1480047)\n"
     ]
    }
   ],
   "source": [
    "print mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNeighborsWhole(distances,train_labels,train_no,test_no, k):\n",
    "    train_no=12500\n",
    "    test_labels=[]\n",
    "\n",
    "    for index in range(train_no, train_no+test_no):\n",
    "        similarity = distances[index, :train_no].toarray().tolist()[0]\n",
    "\n",
    "        zipped_sim_labels = zip(similarity, train_labels, range(len(train_labels)))\n",
    "\n",
    "        sorted_zipped_sim_labels = sorted(zipped_sim_labels, key=lambda (val, k, l): val, reverse=True)\n",
    "        tmp = 0\n",
    "\n",
    "        for j in range(k):\n",
    "            if sorted_zipped_sim_labels[j][0] > 0:\n",
    "                tmp += int(sorted_zipped_sim_labels[j][1])\n",
    "        if tmp == 0:\n",
    "            #get nearest one in case of tie\n",
    "            tmp = np.random.randint(-1,2)\n",
    "        if tmp > 0:\n",
    "            test_labels.append('+1')\n",
    "        else:\n",
    "            test_labels.append('-1')\n",
    "   \n",
    "    return test_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances = cosine_similarity(mat,dense_output=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating for K=17\n",
      "17--->0.7872\n",
      "calculating for K=33\n",
      "33--->0.80472\n",
      "calculating for K=349\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-6918a7c1d49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m        \u001b[0;31m# predict the class of each test sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m    \u001b[0;32mprint\u001b[0m \u001b[0;34m\"calculating for K=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m    \u001b[0mclspr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNeighborsWhole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m    \u001b[0;31m#print clspr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-e484fe72ab85>\u001b[0m in \u001b[0;36mgetNeighborsWhole\u001b[0;34m(distances, train_labels, train_no, test_no, k)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtrain_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mzipped_sim_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msorted_zipped_sim_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipped_sim_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "    \n",
    "def classify(x, train, clstr,k=3):\n",
    "        r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "        \"\"\"\n",
    "        # find nearest neighbors for x\n",
    "        dots = x.dot(train.T)\n",
    "        sims = list(zip(dots.indices, dots.data))\n",
    "        if len(sims) == 0:\n",
    "            # could not find any neighbors\n",
    "            return '+' if np.random.rand() > 0.5 else '-'\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        tc = Counter(clstr[s[0]] for s in sims[:k]).most_common(2)\n",
    "        if len(tc) < 2 or tc[0][1] > tc[1][1]:\n",
    "            # majority vote\n",
    "            return tc[0][0]\n",
    "        # tie break\n",
    "        tc = defaultdict(float)\n",
    "        for s in sims[:k]:\n",
    "            tc[clstr[s[0]]] += s[1]\n",
    "        return sorted(tc.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        \n",
    "\n",
    "        \n",
    "macc = 0.0\n",
    "d=10\n",
    "\n",
    "\n",
    "\n",
    "#svd = TruncatedSVD(n_components=5000, n_iter=7, random_state=42)\n",
    "\n",
    "#scaled_mat = svd.fit_transform(mat)\n",
    "\n",
    "\n",
    "k_list =[17,33,349,399,449]\n",
    "#k_list =[449]\n",
    "\n",
    "\n",
    "\n",
    "for f in k_list:\n",
    "    \n",
    "\n",
    "        # split data into training and testing\n",
    "        # predict the class of each test sample\n",
    "    print \"calculating for K=\"+str(f)\n",
    "    clspr = getNeighborsWhole(distances, train_labels,len(train_documents),len(test_documents),f)\n",
    "    #print clspr\n",
    "\n",
    "    #clspr = [ classify(test[i,:], train, clstr,f) for i in range(test.shape[0]) ]\n",
    "        # compute the accuracy of the prediction\n",
    "    acc = 0.0\n",
    "    for i in range(len(test_labels)):\n",
    "        if test_labels[i] == clspr[i]:\n",
    "            acc += 1\n",
    "    acc /= len(test_labels)\n",
    "    print str(f)+\"--->\"+str(acc)\n",
    "    macc += acc\n",
    "        \n",
    "print macc/(31-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "import os\n",
    "filename = 'output.dat'\n",
    "highscore = open(filename,'w')\n",
    "num_rows, num_cols = tmat.shape\n",
    "print num_rows\n",
    "\n",
    "clspr = getNeighborsWhole(mat,tmat,99)\n",
    "\n",
    "for e in clspr:\n",
    "    \n",
    "    highscore.write(e+\"\\n\")\n",
    "\n",
    "highscore.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from itertools import product\n",
    "from string import ascii_lowercase\n",
    "keywords = map(''.join, product(\"hello\", repeat=3))\n",
    "print keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat1 =[[1,2][3,4]]\n",
    "mat2 =[[8,9],[5,7],[1,2]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "distances=mat1.dot(mat2.T)\n",
    "distances =np.array(distances)\n",
    "indixes= np.argsort(distances,axis=1)[::-1]\n",
    "\n",
    "print distances\n",
    "\n",
    "print indixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat1 =np.array([[1,90],[3,4]])\n",
    "mat2 =np.array([[8,9],[5,7],[1,2],[6,7],[8,4]])\n",
    "train_labels=['-1','+1','-1','+1','-1']\n",
    "\n",
    "clspr = getNeighborsWhole(mat2,mat1,3)\n",
    "\n",
    "print clspr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group words for different K values for K-mer implementation\n",
    "def grouper(input_list, n = 2):\n",
    "    for i in xrange(len(input_list) - (n - 1)):\n",
    "        yield input_list[i:i+n]\n",
    "\n",
    "def get_k_mers(input_list):\n",
    "    new_list = []\n",
    "    #new_list.extend(input_list)\n",
    "    for first, second in grouper(input_list, 2):\n",
    "        st = first + \" \"+second\n",
    "        new_list.append(st)\n",
    "\n",
    "    for first, second, third in grouper(input_list, 3):\n",
    "        st = first + \" \"+second + \" \"+third\n",
    "        new_list.append(st)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[\"abcde\",\"rashmi\",\"popopopoop\",\"whst i \"]\n",
    "\n",
    "\n",
    "get_k_mers(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=[]\n",
    "l.append([\"abcde\",\"rashmi\",\"popopopoop\",\"whst i \",\"this\",\"ss\"])\n",
    "l.append([\"abcde\",\"rashmi\",\"popopopoop\",\"whst i \",\"this\"])\n",
    "print 'this'  in l[0]\n",
    "\n",
    "build_matrix(l,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
