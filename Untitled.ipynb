{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training file\n",
      "Processing test file\n",
      "Building CSR matrix\n",
      "Computing cosine similarity\n",
      "Computing KNN for test reviews\n",
      "Classification report for k =  21\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.82      0.75      0.79      6253\n",
      "          1       0.77      0.84      0.80      6247\n",
      "\n",
      "avg / total       0.80      0.80      0.80     12500\n",
      "\n",
      "Count for k  9953\n",
      "Classification report for k =  22\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.82      0.75      0.79      6253\n",
      "          1       0.77      0.84      0.80      6247\n",
      "\n",
      "avg / total       0.80      0.80      0.80     12500\n",
      "\n",
      "Count for k  9948\n",
      "Classification report for k =  23\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.76      0.79      6253\n",
      "          1       0.77      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.80      0.80      0.80     12500\n",
      "\n",
      "Count for k  9973\n",
      "Classification report for k =  24\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.82      0.75      0.79      6253\n",
      "          1       0.77      0.84      0.80      6247\n",
      "\n",
      "avg / total       0.80      0.79      0.79     12500\n",
      "\n",
      "Count for k  9931\n",
      "Classification report for k =  25\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.76      0.79      6253\n",
      "          1       0.77      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.80      0.80      0.80     12500\n",
      "\n",
      "Count for k  9978\n",
      "Classification report for k =  26\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.82      0.75      0.79      6253\n",
      "          1       0.77      0.83      0.80      6247\n",
      "\n",
      "avg / total       0.80      0.79      0.79     12500\n",
      "\n",
      "Count for k  9927\n",
      "Classification report for k =  27\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.76      0.79      6253\n",
      "          1       0.78      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.80      0.80      0.80     12500\n",
      "\n",
      "Count for k  10025\n",
      "Classification report for k =  28\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.76      0.79      6253\n",
      "          1       0.78      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.80      0.80      0.80     12500\n",
      "\n",
      "Count for k  10018\n",
      "Classification report for k =  29\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.76      0.80      6253\n",
      "          1       0.78      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.80      0.80      0.80     12500\n",
      "\n",
      "Count for k  10037\n",
      "Classification report for k =  30\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.78      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.80      0.80     12500\n",
      "\n",
      "Count for k  10045\n",
      "Classification report for k =  31\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.78      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.80      0.80     12500\n",
      "\n",
      "Count for k  10050\n",
      "Classification report for k =  32\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.78      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.80      0.80     12500\n",
      "\n",
      "Count for k  10051\n",
      "Classification report for k =  33\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.79      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.81      0.81     12500\n",
      "\n",
      "Count for k  10066\n",
      "Classification report for k =  34\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.79      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.81      0.81     12500\n",
      "\n",
      "Count for k  10080\n",
      "Classification report for k =  35\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.79      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.81      0.81     12500\n",
      "\n",
      "Count for k  10076\n",
      "Classification report for k =  36\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.79      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.81      0.81     12500\n",
      "\n",
      "Count for k  10071\n",
      "Classification report for k =  37\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.79      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.81      0.81     12500\n",
      "\n",
      "Count for k  10088\n",
      "Classification report for k =  38\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.79      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.81      0.81     12500\n",
      "\n",
      "Count for k  10071\n",
      "Classification report for k =  39\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.78      0.80      6253\n",
      "          1       0.79      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.81      0.81     12500\n",
      "\n",
      "Count for k  10095\n",
      "Classification report for k =  40\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.83      0.77      0.80      6253\n",
      "          1       0.79      0.84      0.81      6247\n",
      "\n",
      "avg / total       0.81      0.81      0.81     12500\n",
      "\n",
      "Count for k  10095\n",
      "Best k value  39\n",
      "Processing training file\n",
      "Processing test file\n",
      "Building CSR matrix\n",
      "Computing cosine similarity\n",
      "Computing KNN for test reviews\n",
      "Test label length\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "K nearest neighbor classifier for Amazon reviews classification\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import norm\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import metrics\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "print_reviews = False\n",
    "print_debug = True\n",
    "mode1 = \"evaluation\"\n",
    "mode2 = \"test\"\n",
    "evaluation_k = [21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]\n",
    "\n",
    "def define_files (mode, k):\n",
    "    k_values = k\n",
    "    if mode == mode1:\n",
    "        actual_train_file = \"train.dat\"\n",
    "        train_file = \"sample_train.dat\"\n",
    "        test_file = \"evaluation.dat\"\n",
    "        return actual_train_file, train_file, test_file, k_values\n",
    "\n",
    "    elif mode == mode2:\n",
    "        train_file = \"train.dat\"\n",
    "        test_file = \"test.dat\"\n",
    "        result_file = \"format.dat\"\n",
    "        return train_file, test_file, result_file, k_values\n",
    "\n",
    "    else:\n",
    "        print \"Mode is invalid \\n\\n\"\n",
    "        # exit\n",
    "\n",
    "\n",
    "def prepare_files(actual_train_file, sample_train_file, sample_evaluation_file):\n",
    "    with open(actual_train_file, \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "\n",
    "    random.shuffle(lines)\n",
    "    train_lines = lines[0:len(lines)/2]\n",
    "    evaluation_lines = lines[len(lines)/2:]\n",
    "\n",
    "    # Remove all rating info from evaluation file\n",
    "    evaluation_labels = [int(l[:2]) for l in evaluation_lines]\n",
    "\n",
    "    modified_evaluation_lines = []\n",
    "    for t in evaluation_lines:\n",
    "        t = t[2:]\n",
    "        modified_evaluation_lines.append(t)\n",
    "\n",
    "    # Create new test and train files\n",
    "    train = open(sample_train_file, 'w')\n",
    "    for t in train_lines:\n",
    "        train.write(t)\n",
    "    train.close()\n",
    "\n",
    "    test = open(sample_evaluation_file, 'w')\n",
    "    for t in modified_evaluation_lines:\n",
    "        test.write(t)\n",
    "    test.close()\n",
    "\n",
    "    return evaluation_labels\n",
    "\n",
    "\n",
    "def preprocess(train_file, test_file, result_file, evaluation_labels, mode, k_values):\n",
    "\n",
    "    if print_debug is True:\n",
    "        print 'Processing training file'\n",
    "    # 1. Open train file\n",
    "    with open(train_file, \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "\n",
    "    # 2. Separate the labels\n",
    "    train_labels = [int(l[:2]) for l in lines]\n",
    "\n",
    "    # 3. Remove special characters and convert all words to lowercase\n",
    "    train_docs = [re.sub(r'[^\\w]', ' ',l[2:].lower()).split() for l in lines]\n",
    "\n",
    "    # 4. Remove words with length less than 4\n",
    "    train_reviews_1 = filterLen(train_docs, 4)\n",
    "    train_reviews = stemDoc(train_reviews_1)\n",
    "    for t in train_reviews:\n",
    "        new_list = get_k_mers(t)\n",
    "        t.extend(new_list)\n",
    "    #print t\n",
    "    num_train_samples = len(train_reviews)\n",
    "\n",
    "    if print_debug is True:\n",
    "        print 'Processing test file'\n",
    "    # 5. Repeat above steps with test data\n",
    "    with open(test_file, \"r\") as fh:\n",
    "        test_lines = fh.readlines()\n",
    "    test_docs = [re.sub(r'[^\\w]', ' ',l.lower()).split() for l in test_lines]\n",
    "    test_reviews_1 = filterLen(test_docs, 4)\n",
    "    test_reviews = stemDoc(test_reviews_1)\n",
    "    for t in test_reviews:\n",
    "        new_list = get_k_mers(t)\n",
    "        t.extend(new_list)\n",
    "    num_test_samples = len(test_reviews)\n",
    "\n",
    "    # 6. Combine train_reviews and test_reviews\n",
    "    train_reviews.extend(test_reviews)\n",
    "\n",
    "    if print_debug is True:\n",
    "        print 'Building CSR matrix'\n",
    "    # 7. Build csr_matrix with train and test reviews\n",
    "    csr_mat = build_matrix(train_reviews)\n",
    "\n",
    "    # 8. Decrease importance of popular words with IDF and\n",
    "    # Normalize the matrix to simplify cosine similarity calculation\n",
    "    mat1 = csr_idf(csr_mat, copy=True)\n",
    "    mat = csr_l2normalize(mat1, copy=True)\n",
    "\n",
    "    if print_debug is True:\n",
    "        print 'Computing cosine similarity'\n",
    "    # 8.1 Compute cosine similarity on sparse matrix\n",
    "    similarities_sparse = cosine_similarity(mat,dense_output=False)\n",
    "    # print('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))\n",
    "\n",
    "    if print_debug is True:\n",
    "        print 'Computing KNN for test reviews'\n",
    "    # 9. For each test review find the k-nearest neighbor\n",
    "    all_test_labels = []\n",
    "    for k in k_values:\n",
    "        test_labels = []\n",
    "        for test_review_index in range(num_train_samples, num_train_samples+num_test_samples):\n",
    "            similarity = similarities_sparse[test_review_index, :num_train_samples].toarray().tolist()[0]\n",
    "            # append with label information\n",
    "            similarity_with_labels = zip(similarity, train_labels, range(len(train_labels)))\n",
    "\n",
    "            '''\n",
    "            if test_review_index % 500 == 0:\n",
    "                print_reviews = True\n",
    "            else:\n",
    "                print_reviews = False\n",
    "            '''\n",
    "\n",
    "            # sort for k - nearest\n",
    "            sorted_similarity_with_labels = sorted(similarity_with_labels, key=lambda (val, k, l): val, reverse=True)\n",
    "            if print_reviews:\n",
    "                print 'Computing labels through nearet neighbor for review:\\n', test_reviews[test_review_index-num_train_samples], '\\n\\n Found following nearest reviews'\n",
    "            # Choose top k values from each of the sorted list and find test label\n",
    "            tmp = 0\n",
    "\n",
    "            for j in range(k):\n",
    "                if sorted_similarity_with_labels[j][0] != 0:\n",
    "                    tmp += int(sorted_similarity_with_labels[j][1])\n",
    "                if print_reviews:\n",
    "                    print train_reviews[sorted_similarity_with_labels[j][2]], sorted_similarity_with_labels[j][1]\n",
    "            if tmp == 0:\n",
    "                while tmp == 0:\n",
    "                    tmp = np.random.randint(-1,2)\n",
    "            if tmp > 0:\n",
    "                test_labels.append(1)\n",
    "                tst = 1\n",
    "            else:\n",
    "                test_labels.append(-1)\n",
    "                tst = -1\n",
    "\n",
    "            if print_reviews:\n",
    "                print 'computed label is: ',tst, '\\n'\n",
    "\n",
    "        all_test_labels.append(test_labels)\n",
    "\n",
    "    # Check accuracy of labels with different k values\n",
    "    if mode == mode1:\n",
    "        temp = []\n",
    "        count = []\n",
    "        for idx, a in enumerate(all_test_labels):\n",
    "            print \"Classification report for k = \", evaluation_k[idx]\n",
    "            print metrics.classification_report(evaluation_labels, a)\n",
    "            n = 0\n",
    "            temp = np.isclose(a, evaluation_labels)\n",
    "            for t in temp:\n",
    "                if t == True:\n",
    "                    n = n + 1\n",
    "            count.append(n)\n",
    "            print \"Count for k \", n\n",
    "        # Return the best k value\n",
    "        if print_debug is True:\n",
    "            print 'Best k value ', evaluation_k[count.index(max(count))]\n",
    "        return evaluation_k[count.index(max(count))]\n",
    "\n",
    "    elif mode == mode2:\n",
    "        # Write the Best k test labels to format.dat file\n",
    "        target = open(result_file, 'w')\n",
    "        for t in all_test_labels[0]:\n",
    "            if t == 1:\n",
    "                target.write(\"+1\")\n",
    "            else:\n",
    "                target.write(\"-1\")\n",
    "            target.write(\"\\n\")\n",
    "        target.close()\n",
    "\n",
    "    else:\n",
    "        print \"Wrong mode! Inside preprocess\"\n",
    "\n",
    "    return test_labels\n",
    "\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents,\n",
    "    each of which is a list of word/terms in the document.\n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    # Remove all ratings\n",
    "    for d in docs:\n",
    "        #d = d[1:]\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        #d = d[1:]\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "\n",
    "    return mat\n",
    "\n",
    "#@profile\n",
    "def filterLen(docs, minlen):\n",
    "    \"\"\" filter out terms that are too short.\n",
    "    docs is a list of lists, each inner list is a document represented as a\n",
    "    list of words minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if len(t) >= minlen ] for d in docs ]\n",
    "\n",
    "def stemDoc(docs):\n",
    "    \"\"\" automatically removes suffixes (and in some cases prefixes) in order to\n",
    "    find the root word or stem of a given word\n",
    "    \"\"\"\n",
    "    return [ [stem(t) for t in d ] for d in docs]\n",
    "\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf.\n",
    "    Returns scaling factors as dict. If copy is True,\n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "\n",
    "    return df if copy is False else mat\n",
    "\n",
    "#@profile\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm.\n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "\n",
    "    if copy is True:\n",
    "        return mat\n",
    "\n",
    "# Group words for different K values for K-mer implementation\n",
    "def grouper(input_list, n = 2):\n",
    "    for i in xrange(len(input_list) - (n - 1)):\n",
    "        yield input_list[i:i+n]\n",
    "\n",
    "def get_k_mers(input_list):\n",
    "    new_list = []\n",
    "    #new_list.extend(input_list)\n",
    "    for first, second in grouper(input_list, 2):\n",
    "        st = first + \" \"+second\n",
    "        new_list.append(st)\n",
    "\n",
    "    for first, second, third in grouper(input_list, 3):\n",
    "        st = first + \" \"+second + \" \"+third\n",
    "        new_list.append(st)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. For evaluation mode to find the best k value\n",
    "    actual_train_file, train_file, evaluation_file, k_values = define_files(mode1, evaluation_k)\n",
    "    evaluation_labels = prepare_files(actual_train_file, train_file, evaluation_file)\n",
    "    k = preprocess(train_file, evaluation_file, \"\",evaluation_labels, mode1, k_values)\n",
    "\n",
    "    best_k = []\n",
    "    best_k.append(k)\n",
    "\n",
    "    #2. Actual testing with original train and test files\n",
    "    train_file, test_file, result_file, k_value = define_files(mode2, best_k)\n",
    "    test_labels = preprocess(train_file, test_file, result_file, [], mode2, k_value)\n",
    "\n",
    "    print 'Test label length'\n",
    "    print len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
